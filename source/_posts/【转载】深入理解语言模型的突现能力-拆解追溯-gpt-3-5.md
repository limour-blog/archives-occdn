---
title: 【转载】深入理解语言模型的突现能力 & 拆解追溯 GPT-3.5 各项能力的起源
tags: []
id: '2585'
categories:
  - - 转载
date: 2023-02-16 13:31:26
---

**根据转载协议，在文章的开头标明出处：[深入理解语言模型的突现能力](https://yaofu.notion.site/514f4e63918749398a1a8a4c660e0d5b)和[拆解追溯 GPT-3.5 各项能力的起源](https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756)转载自[符尧](https://franxyao.github.io/)、[彭昊](https://haopeng-nlp.github.io/)、[Tushar Khot](https://allenai.org/team/tushark)在notion上的文章，翻译[李如寐](https://scholar.google.com/citations?hl=zh-CN&user=zlWAv4gAAAAJ)、[郭志江](https://cartus.github.io/)**。

## 深入理解语言模型的突现能力

注：本文完成于 ChatGPT 上线之前的一个月，当时我意识到大模型非同小可，所以写下本文，希望引起更多人关注到大模型有可能带来的研究范式转变。一个月之后，ChatGPT 上线，全网轰动，范式从此转变。

最近，人们对大型语言模型所展示的强大能力（例如思维链、[便签本](https://lingo.csail.mit.edu/blog/arithmetic_gpt3/)）产生了极大的兴趣，并开展了许多工作。我们将之统称为大模型的突现能力，这些能力可能只存在于大型模型中，而不存在于较小的模型中，因此称为“突现”。其中许多能力都非常令人印象深刻，比如复杂推理、知识推理和分布外鲁棒性，我们将在后面详细讨论。值得注意的是，这些能力很接近 NLP 社区几十年来一直寻求的能力，因此代表了一种潜在的研究范式转变，即从微调小模型到使用大模型进行上下文学习。对于先行者来说，范式转变可能是很显然的。然而，出于科学的严谨性，\*\*我们确实需要非常明确的理由来说明为什么人们应该转向大型语言模型，即使这些模型昂贵、难以使用，并且效果可能一般。\*\*在本文中，我们将仔细研究这些能力是什么，大型语言模型可以提供什么，以及它们在更广泛的 NLP / ML 任务中的潜在优势是什么。

**前提：**我们假设读者具备以下知识：

*   预训练、精调、提示（普通从业者应具备的自然语言处理/深度学习能力）
*   思维链提示、便签本（普通从业者可能不太了解，但不影响阅读）

### 存在于大模型而非小模型的突现能力

![](https://img-cdn.limour.top/i/2023/02/16/63edb6fe00728.png)

图片来自于 Wei. et. al. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models。X 轴为模型大小。GSM8K是是一个小学水平的数学问题集。

在以上的效果图中，我们可以观察到模型的表现：

*   当尺寸相对小的时候提升并不大
*   当模型变大时有很明显的提升

这从根本上说明，某些能力可能不存在于小模型中，而是在大模型中获得的。

有很多种突现能力，比如 Wei 等人在 2022年 所梳理的。有些能力很有意思，但我们在本文不会讨论，比如把一串单词的最后一个字母拼起来，我们认为这是 Python 而不是语言模型要做的任务；或者3位数加法，我们认为这是计算器而不是语言模型要做的事。

在本文中，我们主要对以下能力感兴趣：

*   NLP 社区近几年都关注，但之前的 NLP 模型很难达到的能力
*   源自于人类语言最深层的本质的能力（能力的深度）
*   可能达到人类智力的最高水平的能力（能力的上限）

#### 突现能力的三个典型例子

很多有意思的能力都可以归到上文提到的类别里，在它们之中，我们主要讨论以下三种典型能力：

*   复杂推理
*   知识推理
*   分布外鲁棒性

接下来让我们一个个详细讨论。

#### 复杂推理

下面是一个 GSM8K 数据集中，用提示词显著超过精调的例子：

*   问题： 克莱儿每天早饭都用 3 颗蛋做蛋卷，她 4 周会吃掉多少打鸡蛋？
*   克莱儿每天早饭都用3颗蛋做蛋卷。
*   一周有 7 天。
*   所以她一周会吃 3 \* 7 = 21 颗蛋。
*   她4周会吃掉 4 \* 21 = 84 颗蛋。
*   一打里面是 12 颗蛋。
*   所以 84 / 12 = 7。
*   答案是7。

虽然这道题对于10岁的孩子来说很容易，但对语言模型来说却很难，主要是由于数学和语言混合在一起。

GSM8K 最初由 OpenAI 于 2021 年 10 月提出。当时他们用第一版GPT3在全部训练集上进行了精调，准确率约为 35%。这个结果让作者相当悲观，因为他们的结果显示了语言模型的缩放规律：随着模型大小呈指数增长，性能呈线性增长（我之后会讨论）。因此，他们在第 4.1 节中思考：

**“175B 模型似乎需要至少额外两个数量级的训练数据才能达到 80% 的求解率。”**

三个月后，即 2022 年 1 月，Wei 等人基于 540B PaLM 模型，**仅使用了 8 个思维链提示示例便将准确率提高到 56.6%**（无需将训练集增加两个数量级）。之后在 2022 年 3 月，Wang 等人基于相同的 540B PaLM 模型，通过多数投票的方法将准确率提高到 74.4%。当前的 SOTA 来自我自己在 AI2 的工作（Fu et. al. Nov 2022），我们通过使用复杂的思维链在 175B Codex 上实现了 82.9% 的准确率。从以上进展可以看到，技术进步确实呈指数级增长。

思维链提示是一个展示模型随着规模突现出能力的典型例子：

*   **从突现能力来看**：模型大小确实要大于 100B ，才能使思维链的效果大于的仅有回答提示。所以这种能力只存在于大型模型中。
*   **从效果来看**：思想链提示的性能明显优于其之前的精调方法。
*   **从标注效率上来看**：思维链提示只需要 8 个示例的注释，而微调需要完整的训练集。

有些同学可能会认为模型能做小学数学代表不了什么（从某种意义上说，他们确实没有那么酷）。但 GSM8K 只是一个开始，最近的工作已经把前沿问题推向了高中、大学，甚至是国际数学奥林匹克问题。现在更酷了吗？

#### 知识推理

下一个例子是需要知识的推理能力（例如问答和常识推理）。在这种情况下，对大型模型进行提示不一定优于精调小型模型（哪个模型更好还有待观察）。但是这个情况下的注释效率被放大了，因为：

*   在许多数据集中，为了获得所需的背景/常识知识，（以前很小的）模型需要一个外部语料库/知识图谱来检索，或者需要通过多任务学习在增强的数据上进行训练
*   对于大型语言模型，可以直接去掉检索器，仅依赖模型的内部知识，且无需精调

![](https://img-cdn.limour.top/i/2023/02/16/63edb8ffc6c8e.png)

图片来自于 Yu et. al. 2022. 以前的 SOTA 模型需要从外部知识源中检索。 GPT-3 的性能与以前的模型相当/优于以前的模型，且无需检索。

如表中所示，与数学题的例子不同，GPT-3 并没有明显优于之前的精调模型。但它不需要从外部文档中检索，本身就包含了知识。

为了理解这些结果的重要性，我们可以回顾一下历史：NLP 社区从一开始就面临着**如何有效编码知识**的挑战。人们一直在不断探究把知识保存在模型外部或者内部的方法。上世纪九十年代以来，人们一直试图将语言和世界的规则记录到一个巨大的图书馆中，将知识存储在模型之外。但这是十分困难的，毕竟我们无法穷举所有规则。因此，研究人员开始构建特定领域的知识库，来存储非结构化文本、半结构化（如维基百科）或完全结构化（如知识图谱）等形式的知识。通常，**结构化知识很难构建**（因为要设计知识的结构体系），**但易于推理**（因为有体系结构），**非结构化知识易于构建**（直接存起来就行），**但很难用于推理**（没有体系结构）。然而，语言模型提供了一种新的方法，可以轻松地从非结构化文本中提取知识，并在不需要预定义模式的情况下有效地根据知识进行推理。下表为优缺点对比：

**构建**

**推理**

**结构化知识**

难构建  
需要设计体系结构并解析

容易推理  
有用的结构已经定义好了

**非结构化知识**

容易构建  
只存储文本即可

难推理  
需要抽取有用的结构

**语言模型**

容易构建  
在非结构化文本上训练

容易推理  
使用提示词即可

#### 分布外鲁棒性

我们讨论的第三种能力是分布外的鲁棒性。在 2018 年至 2022 年期间，NLP、CV 和通用机器学习领域有大量关于分布偏移/对抗鲁棒性/组合生成的研究，人们发现当测试集分布与训练分布不同时，模型的行为性能可能会显著下降。然而，在大型语言模型的上下文学习中似乎并非如此。Si 等人在2022年的研究显示：

![](https://img-cdn.limour.top/i/2023/02/16/63edba6ee6996.png)

数据来自于 Si et. al. 2022. 虽然 GPT-3 在同分布设置下比 RoBERTa 要差，但在非同分布设置下优于 RoBERTa，性能下降明显更小。

同样，在此实验中，同分布情况下基于提示词的 GPT-3 的效果并没有精调后的 RoBERTa要好。但它在三个其他分布（领域切换、噪声和对抗性扰动）中优于 RoBERTa，这意味着 GPT3 更加鲁棒。

此外，即使存在分布偏移，好的提示词所带来的泛化性能依旧会继续保持。比如：

![](https://img-cdn.limour.top/i/2023/02/16/63edbaab0a39f.png)

图片来自于 Fu et. al. 2022. 即使测试分布与训练分布不同，复杂提示也始终比简单提示的表现更好。

Fu 等人2022年的研究显示，输入提示越复杂，模型的性能就越好。这种趋势在分布转移的情况下也会继续保持：无论测试分布与原分布不同、来自于噪声分布，或者是从另一个分布转移而来的，复杂提示始终优于简单提示。

#### 到目前为止的总结

在上文中，我讨论了只有大型模型才有的三种突现能力。它们是：

*   复杂推理，大型模型在没有使用全部训练数据的情况下便显著优于以前的小型模型。
*   知识推理，大型模型可能没有小模型效果好，但大模型不需要额外的知识来源（知识可能很昂贵，或者很难从非结构化数据中抽取）。
*   分布外鲁棒性，这是之前进行模型精调时需要努力解决的问题。大型模型虽然在同分布情况下的效果不如以前的方法，但非同分布情况下的泛化性能却好得多。

### 突现能力推翻比例定律

鉴于上文列出的优点，大家可能会开始觉得大型语言模型确实很好了。在进一步讨论之前，让我们再回顾一下之前的工作，就会发现一个很奇怪的问题：GPT-3 在 2020 年就发布了，但为什么直到现在我们才发现并开始思考范式的转变？

这个问题的答案就藏在两种曲线中：对数线性曲线和相变曲线。如下图：

![](https://img-cdn.limour.top/i/2023/02/16/63edbb2bbe373.png)

左图: 比例定律. 当模型大小呈指数增长时，相应的模型性能呈线性增长。右图: 当模型尺寸达到一定规模时，会出现突现能力，让性能急剧增加。

最初，（OpenAI）的研究者认为语言模型的性能与模型尺寸的关系可以通过对数线性曲线预测，即模型尺寸呈指数增长时，性能会随之线性增加。这种现象被称为语言模型的缩放定律，正如 Kaplan 等人在2020年最初的GPT3文章中讨论的那样。重要的是，在那个阶段，即便最大的 GPT-3 在有提示的情况下也不能胜过小模型精调。所以当时并没有必要去使用昂贵的大模型（即使提示词的标注效率很高）。直到2021年，Cobbe 等人发现缩放定律同样适用于精调。这是一个有点悲观的发现，因为它意味着**我们可能被锁定在模型规模上**——虽然模型架构优化可能会在一定程度上提高模型性能，但效果仍会被锁定在一个区间内（对应模型规模），很难有更显著的突破。

在缩放定律的掌控下（2020年到2021），由于GPT-3无法胜过精调 T5-11B，同时T5-11B微调已经很麻烦了，所以NLP社区的关注点更多的是研究更小的模型或者高效参数适应。Prefix tuning就是提示和适应交叉的一个例子，后来由 He 等人在 2021统一。当时的逻辑很简单：**如果精调效果更好，我们就应该在高效参数适应上多下功夫；如果提示词的方法更好，我们应该在训练大型语言模型上投入更多精力。**

之后在 2022 年 1 月，思维链的工作被放出来了。正如作者所展示的那样，思维链提示在性能-比例曲线中表现出明显的**相变**。当模型尺寸足够大时，性能会显著提高并明显超越比例曲线。

当使用思维链进行提示时，大模型在复杂推理上的表现明显优于微调，在知识推理上的表现也很有竞争力，并且分布鲁棒性也存在一定的潜力。要达到这样的效果只需要8个左右的示例，这就是为什么范式可能会转变（注：本文完成于 ChatGPT 上线之前的一个月；在 ChatGPT 上线之后，整个领域为之震撼，意识到范式已经转变了）。

### 范式转变意味着什么？

范式转变究竟意味着什么？下面我们给出精调和提示词方法的对比：

模型规模

小模型

大模型

学习方法

微调

上下文学习

学习范式

监督学习

监督学习？？

数据

完整训练数据集

少量上下文学习样本

泛化

分布内泛化

同时泛化到分布内 + 分布迁移

提示词的好处很明显：我们不再需要繁琐的数据标注和在全量数据上进行精调，只需要编写提示词并获得满足要求的结果，这比精调要快很多。

另外要注意的两点是：

**上下文学习是监督学习吗？**

*   坦白讲，我不确定。
*   相似之处在于，上下文学习也需要像训练数据一样的示例
*   不同之处在于，上下文学习的泛化行为并不同于监督学习，这使得之前的泛化理论（例如 Rademancher Complexity 或 Neural Tangent Kernel）均不适用。

上下文学习真的比监督学习效果要好吗？

*   答案还未知。
*   大多数提示词和精调的对比都只比了 提示词+大模型 vs 精调+小模型，但公平的对比应该是 提示词+大模型 vs 精调+大模型，且对比时的基座模型应该一样。所以在最初的思维链文章中，如果 Wei 等人要说明提示词好于精调，他们应该对比精调后的PaLM，而不是GPT3。
*   我的假设是：**精调可以提高分布内的性能，但会损害分布外的鲁棒性。提示词在分布变化的场景中表现更好，但在同分布场景下不如精调。**
    *   如果假设是真的，那么一个值得研究的问题就是如何在不牺牲其上下文学习能力的情况下进行精调
    *   **注意分布外精调的效果同样会随着模型尺寸变化。**比如 Yang 等人在2022年的工作中，第四张表就显示，Bart-based的分布外泛化能力会下降，但Bart-large则提升。对于大模型，当测试集的分布和训练集相差不大时，同分布的精调效果也应该会提升。

再回顾一下前文提到的的逻辑：如果精调更好，我们应该努力研究如何进行参数高效的优化；如果提示词更好，我们应该努力去训练更好的大型语言模型。

所以，尽管我们相信大型语言模型有巨大的潜力，仍然没有确凿的证据表明精调和提示词哪种方法更好，因此我们不确定范式是否真的应该转变、或应该转变到什么程度。仔细比较这两种范式，使我们对未来有一个清晰的认识，是非常有意义的。我们将更多讨论留到下一篇文章。

### 模型应该多大才够？

两个数字：62B 和 175B。

*   模型至少需要62B，使思维链的效果才能大于标准的提示词方法。
*   模型至少需要175B（GPT3的尺寸），思维链的效果才能大于精调小模型（T5 11B）的效果。

62B这个数字来自于 Chung 等人 2022 年工作的第五张表。

对于所有小于62B的模型，直接用提示词都好于思维链。第一个用思维链更好的模型是 Flan-cont-PaLM 62B 在BBH上的结果。540B的模型使用思维链会在更多任务上得到好的效果，但也不是全部任务都好于精调。另外，理想的尺寸可以小于 540B，在 Suzgun 等人2022年的工作中，作者展示了175B的 InstructGPT 和 175B的 Codex 使用思维链都好于直接用提示词。综合以上结果，我们得到了63B和175B两个数字。所以，如果想要参与这场游戏，首先要有一个大于平均尺寸的模型。

不过，还有其他大型模型在思维链下的表现差了很多，甚至不能学到思维链，比如 OPT、BLOOM 和 GPT-3 的第一个版本。他们的尺寸都是175B。这就引出了我们下一个要讨论的问题。

### 规模是唯一的因素吗？

不是。

规模是一个必要但不充分的因素。有些模型足够大（比如 OPT 和 BLOOM，都是 175B），但并不能做思维链。

有两种模型可以做思维链:

*   GPT3系列的模型，包括 text-davinci-002 和 code-davinci-002 (Codex)。**这是仅有的两个具有强大突现能力并可公开访问的模型。**
    *   除了以上两个模型，其他GPT3模型，包括原来的GPT3，text-davinci-001，以及其他更小的GPT-3模型，都不能做思维链。
    *   当说“能做思维链”时，我们是指使用思维链方法的效果比直接用提示词、精调T5-11B效果更好。
    *   另外要注意的是，code-davinci-002 在**语言**任务上的性能始终优于 text-davinci-002。这个观察非常有趣且耐人寻味。这表明**基于代码数据训练的语言模型可以胜过根据语言训练的语言模型。**目前为止我们还不知道是为什么。
*   PaLM系列模型，包括 PaLM、U-PaLM、Flan-PaLM 和 Minerva。这些模型目前还未开放访问（此处@谷歌，快开源吧）。

为什么会有突现能力目前还不清楚，但我们找出了一下可能产生突现能力的因素：

*   指令精调：GPT-3 text-davinci-002 就是用指令+强化学习精调的产物。在这之前，text-davinci-001 做思维链的效果并不好。同时PaLM在经过指令精调后的效果也有提升。
*   在代码上精调：Codex code-davinci-002 是在代码上进行精调的，它的效果持续好于 text-davinci-002。PaLM 也在代码上进行了调整。从表面上看，代码与语言关系不大，但似乎起了很大作用，我们会在之后的文章进行讨论。
*   用思维链精调：在 text-davinci-002 发布时，谷歌已经发布 PaLM 3 个月了。所以 OpenAI 应该看到了思维链相关的工作。还有一些工作表明，直接用思维链数据进行精调可以激发模型的思维链能力。

然而，所有这些因素在现阶段都是推测。揭示如何训练才能让模型产生突现能力是非常有意义的，我们将更多讨论留到下一篇文章。

### 总结

在本文中，我们仔细研究了语言模型的突现能力。我们强调了复杂推理、知识推理和分布外鲁棒性的重要性和其中存在的机会。突现能力是非常令人兴奋的，因为它们可以超越比例定律，并在比例曲线中表现出相变。我们详细讨论了研究范式是否会真的从精调转向上下文学习，但我们目前还没有确切答案，因为精调和上下文学习在分布内、分布外场景下的效果仍有待对比。最后，我们讨论了产生突现能力的三个潜在因素：指令精调、代码精调和思维链精调。非常欢迎大家提出建议和讨论。

另外我们还提到了两个尚未讨论的有趣问题：

*   我们是否能公平对比精调和上下文学习的效果？
*   我们是如何训练大模型，才能让模型具备突现能力、思维链能力？

对于这两个问题，我们会在之后的文章中进行讨论。

## 拆解追溯 GPT-3.5 各项能力的起源

最近，OpenAI的预训练模型ChatGPT给人工智能领域的研究人员留下了深刻的印象和启发。毫无疑问，它又强又聪明，且跟它说话很好玩，还会写代码。它在多个方面的能力远远超过了自然语言处理研究者们的预期。于是我们自然就有一个问题：ChatGPT 是怎么变得这么强的？它的各种强大的能力到底从何而来？在这篇文章中，我们试图剖析 ChatGPT 的突现能力（Emergent Ability），追溯这些能力的来源，希望能够给出一个全面的技术路线图，来说明 GPT-3.5 模型系列以及相关的大型语言模型是如何一步步进化成目前的强大形态。

我们希望这篇文章能够促进大型语言模型的透明度，成为开源社区共同努力复现 GPT-3.5 的路线图。

“多年以后，面对行刑队，奥雷里亚诺·布恩迪亚上校将会回想起父亲带他去见识冰块的那个遥远的下午”

—— _《百年孤独》_ 加西亚·马尔克斯

### 一、2020 版初代 GPT-3 与大规模预训练

初代GPT-3展示了三个重要能力：

*   **语言生成**：遵循提示词（prompt），然后生成补全提示词的句子 (completion)。这也是今天人类与语言模型最普遍的交互方式。
*   **上下文学习 (in-context learning)**: 遵循给定任务的几个示例，然后为新的测试用例生成解决方案。很重要的一点是，GPT-3虽然是个语言模型，但它的论文几乎没有谈到“语言建模” (language modeling) —— 作者将他们全部的写作精力都投入到了对上下文学习的愿景上，这才是 GPT-3的真正重点。
*   **世界知识 (world knowledge)**：包括事实性知识 (factual knowledge) 和常识 (commonsense)。

那么这些能力从何而来呢？

基本上，以上三种能力都来自于大规模预训练：在有3000亿单词的语料上预训练拥有1750亿参数的模型（ 训练语料的60%来自于 2016 - 2019 的 C4 + 22% 来自于 WebText2 + 16% 来自于Books + 3%来自于Wikipedia）。其中：

*   **语言生成**的能力来自于语言建模的**训练目标** (language modeling)。
*   **世界知识**来自 3000 亿单词的**训练语料库**（不然还能是哪儿呢）。
*   **模型的 1750 亿参数**是为了**存储知识**，Liang et al. (2022) 的文章进一步证明了这一点。 他们的结论是，知识密集型任务的性能与模型大小息息相关。
*   上下文学习的能力来源及为什么上下文学习可以泛化，\*\*仍然难以溯源。\*\*直觉上，这种能力可能来自于同一个任务的数据点在训练时按顺序排列在同一个 batch 中。然而，很少有人研究为什么语言模型预训练会促使上下文学习，以及为什么上下文学习的行为与微调 (fine-tuning) 如此不同。

令人好奇的是，初代的**GPT-3有多强。** 其实比较难确定初代 GPT-3（在 OpenAI API 中被称为`davinci`）到底是“强”还是“弱”。一方面，它合理地回应了某些特定的查询，并在许多数据集中达到了还不错的性能；另一方面，它在许多任务上的**表现还不如 T5 这样的小模型**（参见其原始论文）。在今天（2022 年 12 月）ChatGPT 的标准下，很难说初代的 GPT-3 是“智能的”。Meta 开源的 OPT 模型试图复现初代 GPT-3，但它的能力与当今的标准也形成了尖锐的对比。许多测试过 OPT 的人也认为与现在的`text-davinci-002`相比，该模型确实 “不咋地”。尽管如此，OPT 可能是初代 GPT-3 的一个足够好的开源的近似模型了（根据 OPT 论文和斯坦福大学的 HELM 评估）。

虽然初代的 GPT-3 可能表面上看起来很弱，但后来的实验证明，初代 GPT-3 有着非常强的潜力。这些潜力后来被代码训练、指令微调 (instruction tuning) 和基于人类反馈的强化学习 (reinforcement learning with human feedback, RLHF) 解锁，最终体展示出极为强大的突现能力。

### 二、从 2020 版 GPT-3 到 2022 版 ChatGPT

从最初的 GPT-3 开始，为了展示 OpenAI 是如何发展到ChatGPT的，我们看一下 GPT-3.5 的进化树：

![](https://img-cdn.limour.top/i/2023/02/16/63edbd69acfc5.png)

在 **2020 年 7 月**，OpenAI 发布了模型索引为的 `davinci` 的初代 GPT-3 论文，从此它就开始不断进化。在 **2021 年 7 月**，Codex 的论文发布，其中初始的 Codex 是根据（可能是内部的）120 亿参数的 GPT-3 变体进行微调的。后来这个 120 亿参数的模型演变成 OpenAI API 中的`code-cushman-001`。在 **2022 年 3 月**，OpenAI 发布了指令微调 (instruction tuning) 的论文，其监督微调 (supervised instruction tuning) 的部分对应了`davinci-instruct-beta`和`text-davinci-001`。在 **2022 年 4 月至 7 月的**，OpenAI 开始对`code-davinci-002`模型进行 Beta 测试，也称其为 Codex。然后`code-davinci-002`、`text-davinci-003`和`ChatGPT` 都是从`code-davinci-002`进行指令微调得到的。详细信息请参阅 OpenAI的模型索引文档。

尽管 Codex 听着像是一个只管代码的模型，但`code-davinci-002`可能是最强大的针对**自然语言**的GPT-3.5 变体（优于 `text-davinci-002`和 `-003`）。`code-davinci-002`很可能在文本和代码上都经过训练，然后根据指令进行调整（将在下面解释）。然后**2022 年 5-6 月**发布的`text-davinci-002`是一个基于`code-davinci-002`的有监督指令微调 (supervised instruction tuned) 模型。在`text-davinci-002`上面进行**指令微调**很可能**降低**了模型的**上下文学习**能力\*\*，**但是**增强了**模型的**零样本能力\*\*（将在下面解释）。然后是`text-davinci-003`和 `ChatGPT`，它们都在 **2022 年 11 月**发布，是使用的基于人类反馈的强化学习的版本指令微调 (instruction tuning with reinforcement learning from human feedback) 模型的两种不同变体。`text-davinci-003` 恢复了（但仍然比`code-davinci-002`差）一些在`text-davinci-002` 中丢失的部分**上下文学习能**力（大概是因为它在微调的时候混入了语言建模） 并进一步改进了零样本能力（得益于RLHF）。另一方面，ChatGPT 似乎**牺牲了几乎所有的上下文学习的能力**来**换取**建模对话历史的能力。

总的来说，在 2020 - 2021 年期间，在`code-davinci-002`之前，OpenAI 已经投入了大量的精力通过代码训练和指令微调来增强GPT-3。当他们完成`code-davinci-002`时，所有的能力都已经存在了。很可能后续的指令微调，无论是通过有监督的版本还是强化学习的版本，都会做以下事情（稍后会详细说明）：

*   指令微调**不会为模型注入新的能力** —— 所有的能力都已经存在了。指令微调的作用是**解锁 / 激发这些能力**。这主要是因为指令微调的数据量比预训练数据量少几个数量级（基础的能力是通过预训练注入的）。
*   指令微调\*\*将 GPT-3.5 的分化到不同的技能树。\*\*有些更擅长上下文学习，如`text-davinci-003`，有些更擅长对话，如`ChatGPT`。
*   指令微调**通过牺牲性能换取与人类的对齐（alignment）**。 OpenAI 的作者在他们的指令微调论文中称其为 “对齐税” (alignment tax)。许多论文都报道了`code-davinci-002`在基准测试中实现了最佳性能（但模型不一定符合人类期望）。 在`code-davinci-002`上进行指令微调后，模型可以生成更加符合人类期待的反馈（或者说模型与人类对齐），例如：零样本问答、生成安全和公正的对话回复、拒绝超出模型它知识范围的问题。

### 三、Code-Davinci-002和 Text-Davinci-002，在代码上训练，在指令上微调

在code-davinci-002和text-davinci-002之前，有两个中间模型，分别是 davinci-instruct-beta 和 text-davinci-001。两者在很多方面都比上述的两个-002模型差（例如，text-davinci-001 链式思维推理能力不强）。所以我们在本节中重点介绍 -002 型号。

#### 3.1 复杂推理能力的来源和泛化到新任务的能力

我们关注`code-davinci-002`和`text-davinci-002`，这两兄弟是第一版的 GPT3.5 模型，一个用于代码，另一个用于文本。它们表现出了四种与初代 GPT-3 不同的重要能力：

*   **响应人类指令**：以前，GPT-3 的输出主要训练集中常见的句子。现在的模型会针对指令 / 提示词生成更合理的答案（而不是相关但无用的句子）。
*   **泛化到没有见过的任务**：当用于调整模型的指令数量超过一定的规模时，模型就可以自动在从没见过的新指令上也能生成有效的回答。 **这种能力对于上线部署至关重要**，因为用户总会提新的问题，模型得答得出来才行。
*   **代码生成和代码理解**：这个能力很显然，因为模型用代码训练过。
*   **利用思维链 (chain-of-thought) 进行复杂推理**：初代 GPT3 的模型思维链推理的能力很弱甚至没有。 **code-davinci-002 和 text-davinci-002 是两个拥有足够强的思维链推理能力的模型。**
    *   思维链推理之所以重要，是因为思维链可能是解锁突现能力和超越缩放法则 (scaling laws) 的关键。请参阅上一篇博文。

这些能力从何而来？

与之前的模型相比，两个主要区别是**指令微调**和**代码训练**。具体来说

*   能够**响应人类指令**的能力是**指令微调**的直接产物。
*   **对没有见过的指令做出反馈**的泛化能力是在指令数量超过一定程度之后**自动出现的**，T0、Flan 和 FlanPaLM 论文进一步证明了这一点
*   使用**思维链**进行**复杂推理**的能力很可能是**代码训练**的**一个神奇的副产物**。对此，我们有以下的事实作为一些支持：
    *   最初的 GPT-3 没有接受过代码训练，它不能做**思维链**。
    *   text-davinci-001 模型，虽然经过了指令微调，但第一版思维链论文报告说，它的它思维链推理的能力非常弱 —— **所以指令微调可能不是思维链存在的原因，代码训练才是模型能做思维链推理的最可能原因。**
    *   PaLM 有 5% 的代码训练数据，可以做思维链。
    *   Codex论文中的代码数据量为 159G ，大约是初代 GPT-3 5700 亿训练数据的28%。code-davinci-002 及其后续变体可以做思维链推理。
    *   在 HELM 测试中，Liang et al. (2022) 对不同模型进行了大规模评估。 他们发现了针对代码训练的模型具有很强的语言推理能力，包括 120亿参数的code-cushman-001.。
    *   我们在 AI2 的工作也表明，当配备复杂的思维链时，code-davinci-002 在 GSM8K 等重要数学基准上是目前表现最好的模型
    *   直觉来说，**面向过程的编程 (procedure-oriented programming)** 跟人类**逐步解决任务**的过程很类似，**面向对象编程 (object-oriented programming)** 跟人类**将复杂任务分解为多个简单任务**的过程很类似。
    *   以上所有观察结果都是代码与推理能力 / 思维链 之间的相关性，但不一定是因果性。这种相关性很有趣，但现在还是一个待研究的开放性问题。目前看来，我们**没有非常确凿的证据证明代码就是思维链和复杂推理的原因**。
*   此外， **代码训练**另一个可能的副产品是\*\*长距离依赖，\*\*正如Peter Liu所指出：“语言中的下个词语预测通常是非常局部的，而代码通常需要更长的依赖关系来做一些事情，比如前后括号的匹配或引用远处的函数定义”。这里我想进一步补充的是：由于面向对象编程中的类继承，代码也可能有助于模型建立编码层次结构的能力。我们将对这一假设的检验留给未来的工作。

另外还要注意一些细节差异：

*   **text-davinci-002 与 code-davinci-002**
    *   Code-davinci-002 是基础模型，text-davinci-002 是指令微调 code-davinci-002 的产物（见 OpenAI 的文档）。它在以下数据上作了微调：（一）人工标注的指令和期待的输出；（二）由人工标注者选择的模型输出。
    *   当有上下文示例 (in-context example) 的时候， Code-davinci-002 更擅长上下文学习；当没有上下文示例 / 零样本的时候， text-davinci-002 在零样本任务完成方面表现更好。从这个意义上说，text-davinci-002 更符合人类的期待（因为对一个任务写上下文示例可能会比较麻烦）。
    *   OpenAI 不太可能故意牺牲了上下文学习的能力换取零样本能力 —— 上下文学习能力的降低更多是指令学习的一个副作用，OpenAI 管这叫对齐税。
*   **001 模型（code-cushman-001 和 text-davinci-001）v.s. 002 模型（code-davinci-002 和 text-davinci-002）**
    *   001 模型主要是为了做纯代码 / 纯文本任务； 002 模型则深度融合了代码训练和指令微调，代码和文本都行。
    *   Code-davinci-002 可能是第一个深度融合了代码训练和指令微调的模型。证据有：code-cushman-001 可以进行推理但在纯文本上表现不佳，text-davinci-001 在纯文本上表现不错但在推理上不大行。 code-davinci-002 则可以同时做到这两点。

#### 3.2 这些能力是在预训练之后已经存在还是在之后通过微调注入？

在这个阶段，我们已经确定了指令微调和代码训练的关键作用。一个重要的问题是如何进一步分析代码训练和指令微调的影响？具体来说： 上述三种能力是否**已经存在于初代的GPT-3**中，只是**通过指令和代码训练触发 / 解锁**？ 或者这些能力在初代的 GPT-3 中**并不存在**，是通过指令和代码训练**注入？** 如果答案已经在初代的 GPT-3 中，**那么这些能力也应该在 OPT 中。 因此，要复现这些能力，或许可以直接通过指令和代码调整 OPT。** 但是，code-davinci-002 也可能不是基于最初的 GPT-3 davinci，而是基于比初代 GPT-3 更大的模型。如果是这种情况，可能就没办法通过调整 OPT 来复现了。研究社区需要进一步弄清楚 OpenAI 训练了什么样的模型作为 code-davinci-002 的基础模型。

我们有以下的假设和证据：

*   code-davinci-002的**基础模型可能不是初代GPT-3 davinci 模型**。以下是证据：
    *   初代的GPT-3在数据集 C4 2016 - 2019 上训练，而 code-davinci-002 训练集则在延长到2021年才结束。因此 code-davinci-002 有可能在 C4 的 2019-2021 版本上训练。
    *   初代的 GPT-3 有一个大小为 **2048** 个词的上下文窗口。code-davinci-002 的上下文窗口则为 **8192**。GPT 系列使用绝对位置嵌入 (absolute positional embedding)，直接对绝对位置嵌入进行外推而不经过训练是比较难的，并且会严重损害模型的性能（参考 Press et al., 2022）。如果 code-davinci-002 是基于初代GPT-3，那OpenAI 是如何扩展上下文窗口的？
*   另一方面，无论基础模型是初代的 GPT-3 还是后来训练的模型， **遵循指令和零样本泛化的能力都可能已经存在于基础模型**中，后来才通过指令微调来**解锁** （**而不是注入）**
    *   这主要是因为 OpenAI 的论文报告的指令数据量大小只有 77K，比预训练数据少了几个数量级。
    *   其他指令微调论文进一步证明了数据集大小对模型性能的对比，例如 Chung et al. (2022) 的工作中， Flan-PaLM 的指令微调仅为预训练计算的 0.4%。一般来说，指令数据会显著少于预训练数据。
*   然而 **，模型的复杂推理能力可能是在预训练阶段通过代码数据注入**
    *   代码数据集的规模与上述指令微调的情况不同。这里的代码数据量足够大，可以占据训练数据的重要部分（例如，PaLM 有 8% 的代码训练数据）
    *   如上所述，在 code-davinci-002 之前的模型 text-davinci-001 大概没有在代码数据上面微调过，所以它的推理 / 思维链能力是非常差的，正如第一版思维链论文中所报告的那样，有时甚至比参数量更小的 code-cushman-001 还差。
*   **区分代码训练和指令微调效果的最好方法**可能是**比较 code-cushman-001、T5 和 FlanT5**
    *   因为它们具有相似的模型大小（110亿 和 120亿），相似的训练数据集 (C4)，它们最大的区别就是有没有在代码上训练过 / 有没有做过指令微调。
    *   目前还没有这样的比较。我们把这个留给未来的研究。

### 四、text-davinci-003 和 ChatGPT，基于人类反馈的强化学习(Reinforcement Learning from Human Feedback, RLHF) 的威力

在当前阶段（2022 年 12 月）， text-davinci-002、text-davinci-003 和 ChatGPT之间**几乎没有严格的统计上的比较** ，主要是因为

*   text-davinci-003 和 ChatGPT 在撰写本文时才发布不到一个月。
*   ChatGPT 不能通过 OpenAI API 被调用，所以想要在标准基准上测试它很麻烦。

所以在这些模型之间的比较更多是**基于研究社区的集体经验** （统计上不是很严格）。不过，我们相信初步的描述性比较仍然可以揭示模型的机制。

我们首先注意到以下 text-davinci-002，text-davinci-003 和 ChatGPT 之间的比较：

*   所有三个模型都经过**指令微调**。
*   **text-davinci-002** 是一个经过**监督学习指令微调** (supervised instruction tuning) \*\*\*\*的模型
*   **text-davinci-003 和 ChatGPT** 是**基于人类反馈的强化学习的指令微调** (Instruction tuning with Reinforcement Learning from Human Feedback, RLHF)。这是它们之间最显着的区别。

**这意味着大多数新模型的行为都是 RLHF 的产物**。

那么让我们看看 RLHF 触发的能力：

*   **翔实的回应：** text-davinci-003 的生成通常比 text-davinci-002长。 ChatGPT 的回应则更加冗长，以至于用户必须明确要求“用一句话回答我”，才能得到更加简洁的回答。这是 RLHF 的直接产物。
*   **公正的回应：**ChatGPT 通常对涉及多个实体利益的事件（例如政治事件）给出非常平衡的回答。这也是RLHF的产物。
*   **拒绝不当问题：**这是内容过滤器和由 RLHF 触发的模型自身能力的结合，过滤器过滤掉一部分，然后模型再拒绝一部分。
*   **拒绝其知识范围之外的问题：**例如，拒绝在2021 年 6 月之后发生的新事件（因为它没在这之后的数据上训练过）。这是 RLHF 最神奇的部分，因为它使模型能够隐式地区分哪些问题在其知识范围内，哪些问题不在其知识范围内。

有两件事情值得注意：

*   所有的能力都是模型本来就有的， **而不是通过RLHF 注入的**。 RLHF 的作用是**触发 / 解锁突现能力**。这个论点主要来自于数据量大小的比较：因为与预训练的数据量相比，RLHF 占用的计算量 / 数据量要少得多。
*   模型**知道它不知道什么不是通过编写规则来实现的，** 而是通过RLHF解锁的。这是一个非常令人惊讶的发现，因为 RLHF 的最初目标是让模型生成符合人类期望的回答，这更多是让模型生成安全的句子，而不是让模型知道它不知道的内容。

幕后发生的事情可能是：

*   ChatGPT: 通过**牺牲上下文学习**的能力**换取建模对话历史**的能力。这是一个基于经验的观测结果，因为 ChatGPT 似乎不像 text-davinci-003 那样受到上下文演示的强烈影响。
*   text-davinci-003：**恢复了** text-davinci-002 所牺牲的**上下文学习能力**， **提高零样本的能力**。 我们不确定这是否也是 RLHF 或其他东西的副产品。 根据instructGPT的论文，这是来自于强化学习调整阶段混入了语言建模的目标（而不是 RLHF 本身）。

### 五、总结当前阶段 GPT-3.5 的进化历程

到目前为止，我们已经仔细检查了沿着进化树出现的所有能力，下表总结了演化路径：

能力

OpenAI模型

训练方法

OpenAI API

OpenAI论文

近似的开源模型

GPT-3系列

语言生成 + 世界知识 + 上下文学习

GPT-3初始版本 \*\*大部分的能力已经存在于模型中，尽管表面上看起来很弱。

语言建模

Davinci

GPT-3论文

Meta OPT

\+ 遵循人类的指令 + 泛化到没有见过的任务

Instruct-GPT初始版本

指令微调

Davinci-Instruct-Beta

Instruct-GPT论文

T0论文Google FLAN论文

\+ 代码理解 + 代码生成

Codex初始版本

在代码上进行训练

Code-Cushman-001

Codex论文

Salesforce CodeGen

GPT-3.5系列

++ 代码理解 ++ 代码生成 ++ 复杂推理 / 思维链 (为什么?) + 长距离的依赖 (很可能)

现在的Codex \*\*GPT3.5系列中最强大的模型

在代码+文本上进行训练 在指令上进行微调

Code-Davinci-002 (目前免费的版本 = 2022年12月)

Codex 论文

++ 遵循人类指令 - 上下文学习 - 推理能力 ++ 零样本生成

有监督的Instruct-GPT \*\*通过牺牲上下文学习换取零样本生成的能力

监督学习版的指令微调

Text-Davinci-002

Instruct-GPT论文, 有监督的部分

T0论文 Google FLAN论文

\+ 遵循人类价值观 + 包含更多细节的生成 + 上下文学习 + 零样本生成

经过RLHF训练的Instruct-GPT \*\*和002模型相比，和人类更加对齐，并且更少的性能损失

强化学习版的指令微调

Text-Davinci-003

Instruct-GPT论文, RLHF部分，从人类反馈中的学习摘要。

DeepMind Sparrow 论文AI2 RL4LMs

++ 遵循人类价值观 ++ 包含更多细节的生成 ++ 拒绝知识范围外的问题 (为什么?) ++ 建模对话历史的能力 -- 上下文学习

ChatGPT \*\* 通过牺牲上下文学习的能力换取建模对话历史的能力

使用对话数据进行强化学习指令微调

DeepMind Sparrow论文 AI2 RL4LMs

我们可以得出结论：

*   语言生成能力 + 基础世界知识 + 上下文学习都是来自于预训练（`davinci`）
*   存储大量知识的能力来自 1750 亿的参数量。
*   遵循指令和泛化到新任务的能力来自于扩大指令学习中指令的数量（`Davinci-instruct-beta`)
*   执行复杂推理的能力很可能来自于代码训练（`code-davinci-002`）
*   生成中立、客观的能力、安全和翔实的答案来自与人类的对齐。具体来说：
    *   如果是监督学习版，得到的模型是`text-davinci-002`
    *   如果是强化学习版 (RLHF) ，得到的模型是`text-davinci-003`
    *   无论是有监督还是 RLHF ，模型在很多任务的性能都无法超过 code-davinci-002 ，这种因为对齐而造成性能衰退的现象叫做对齐税。
*   对话能力也来自于 RLHF（`ChatGPT`），具体来说它牺牲了上下文学习的能力，来换取：
    *   建模对话历史
    *   增加对话信息量
    *   拒绝模型知识范围之外的问题

### 六、GPT-3.5 目前不能做什么

虽然GPT-3.5是自然语言处理研究中的重要一步，但它并没有完全包含许多研究人员（包括 AI2）设想的所有理想属性。以下是GPT-3.5不具备的某些重要属性：

*   **实时改写模型的信念**：当模型表达对某事的信念时，如果该信念是错误的，我们可能很难纠正它：
    *   我最近遇到的一个例子是：ChatGPT 坚持认为 3599 是一个质数，尽管它承认 3599 = 59 \* 61。另外，请参阅Reddit上关于游得最快的海洋哺乳动物的例子。
    *   然而，模型信念的强度似乎存在不同的层次。一个例子是即使我告诉它达斯·维达（星球大战电影中的人物）赢得了2020年大选，模型依旧会认为美国现任总统是拜登。但是如果我将选举年份改为 2024 年，它就会认为总统是达斯·维达是 2026 年的总统。
*   **形式推理**：GPT-3.5系列不能在数学或一阶逻辑等形式严格的系统中进行推理：
    *   在自然语言处理的文献中， “推理” 一词的定义很多时候不太明确。但如果我们从模糊性的角度来看，例如一些问题 (a) 非常模棱两可，没有推理；(b) 有点儿逻辑在里面，但有些地方也可以模糊；(c) 非常严谨，不能有任何歧义。那么，
    *   模型可以很好地进行 (b) 类的带模糊性的推理，例子有：
        *   生成如何做豆腐脑的方法。做豆腐脑的时候，中间很多步骤模糊一点是可以接受的，比如到底是做咸的还是做甜的。只要整体步骤大致正确，做出来的豆腐脑儿就能吃。
        *   数学定理的证明思路。证明思路是用语言表达的非正式的逐步解法，其中每一步的严格推导可以不用太具体。证明思路经常被用到数学教学：只要老师给一个大致正确的整体步骤，学生就可以大概明白。然后老师把具体的证明细节作为作业布置给学生，答案略。
    *   GPT-3.5 不能进行类型 (c) 的推理（推理不能容忍歧义）。
        *   一个例子是严格的数学证明，要求中间步骤中不能跳，不能模糊，不能错。
        *   但这种严格推理到底是应该让语言模型做还是让符号系统做还有待讨论。一个例子是，与其努力让 GPT 做三位数加法，不如直接调 Python。
*   **从互联网进行检索**：GPT-3.5 系列（暂时）不能直接搜索互联网
    *   但是有一篇 WebGPT 论文发表于2021年12月，里面就让 GPT 调用了搜索引擎。所以检索的能力已经在 OpenAI 内部进行了测试。
    *   这里需要区分的一点是，GPT-3.5 的两个重要但不同的能力是 **知识** 和 **推理**。一般来说，如果我们能够 **将知识部分卸载到外部的检索系统，让语言模型只专注于推理，这就很不错了。** 因为：
        *   模型的内部知识总是在某个时间被切断。模型始终需要最新的知识来回答最新的问题。
        *   回想一下，我们已经讨论过 1750 亿的参数大量用于存储知识。如果我们可以将知识卸载到模型之外，那么模型参数可能会大大减少，最终它甚至可以在手机上运行（疯狂的想法，但 ChatGPT 已经足够科幻了，谁知道未来会怎样呢).

### 七、结论

在这篇博文中，我们仔细检查了GPT-3.5系列的能力范围，并追溯了它们所有突现能力的来源。初代GPT-3模型通过预训练获得生成能力、世界知识和in-context learning。然后通过instruction tuning的模型分支获得了遵循指令和能泛化到没有见过的任务的能力。经过代码训练的分支模型则获得了代码理解的能力，作为代码训练的副产品，模型同时潜在地获得了复杂推理的能力。结合这两个分支，code-davinci-002似乎是具有所有强大能力的最强GPT-3.5模型。接下来通过有监督的instruction tuning和 RLHF通过牺牲模型能力换取与人类对齐，即对齐税。 RLHF 使模型能够生成更翔实和公正的答案，同时拒绝其知识范围之外的问题。

我们希望这篇文章能够帮助提供一个清晰的GPT评估图，并引发一些关于语言模型、instruction tuning和code tuning的讨论。最重要的是， **我们希望这篇文章可以作为在开源社区内复现GPT-3.5的路线图。**

“因为山就在那里。”

——乔治·马洛里，珠穆朗玛峰探险先驱

### 常见问题

*   这篇文章中的这些说法更像是假设 (hypothesis) 还是结论 (conclusion)？
    *   **复杂推理的能力来自于代码训练**是我们倾向于相信的假设 (hypothesis)
    *   **对没有见过的任务泛化能力来自大规模指令学习** 是至少 4 篇论文的结论 (conclusion)
    *   **GPT-3.5来自于其他大型基础模型，而不是1750亿参数的GPT-3** 是有根据的猜测 (educated guess)。
    *   **所有这些能力都已经存在了，通过instruction tuning，无论是有监督学习或强化学习的方式来解锁而不是注入这些能力** 是一个比较强的假设 (strong assumption)。 主要是因为instruction tuning数据量比预训练数据量少了几个数量级。
    *   结论 (conclusion) = 许多证据支持这些说法的正确性；假设 (hypothesis) = 有正面证据但不够有力；有根据的猜测 (educated guess) = 没有确凿的证据，但某些因素会指向这个方向
*   为什么其他模型（如 OPT 和 BLOOM）没有那么强大？
    *   OPT大概是因为训练过程太不稳定
    *   BLOOM的情况则未知。如果您有更多意见，请与我联系