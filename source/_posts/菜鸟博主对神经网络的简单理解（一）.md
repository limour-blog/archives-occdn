---
title: 菜鸟博主对神经网络的简单理解（一）
tags: []
id: '2161'
categories:
  - - 统计学
comments: false
date: 2022-07-28 22:06:33
---

博主最近在看临床预测模型，里面涉及到一些神经网络的相关知识，这里记录一下博主的简单理解。

## 基本概念

*   `X`：一个nx维的输入张量，可以是nx=0的标量，nx=1的向量，nx=2的矩阵，或更高维的张量
*   `Y`：一个ny维的输出张量，可以是nx=0的标量，nx=1的向量，nx=2的矩阵，或更高维的张量
*   `f`：一个特定结构的[神经网络](https://zhuanlan.zhihu.com/p/159305118)，如简单的BP神经网络、复杂的深度神经网络**DNN**等，用于将X映射到Y
*   `W`：一个nw维的张量，用来代表f中的所有权重，nw与f、X、Y有关，具体多少不用管
*   `L`：损失函数，可以是交叉熵、残差平方等，用来计算f(X)与实际Y的差距

一个简单的例子：假设`f`是DNN，`X`是一个256\*256图片的像素矩阵，`Y`是一个3维分类向量，那么`f`就可以用于将一张特定的256\*256图片`X0`映射到一个特定的`Y0(P(🐱), P(🐕), P(🐖))`，那么我们输出`(🐱,🐕,🐖)[which.max(Y0)]`，就可以对图片`X0`进行`(🐱,🐕,🐖)`的分类。

## 对训练的理解

假设我们有一个训练好的权重`W`，那么我们就可以得到任意`X`下对实际`Y`的良好估计`f(X,W)`,有`L(f(X,W),Y)`相对较小。我们训练的目的，就是根据已有的数据集`D{(X,Y)}`，通过适当的拟合来找到这样一个权重`W`，它可以实现前面假设中的效果。

那么如何进行适当的拟合呢？一个简单的思路是梯度下降法。根据定义，我们知道`L`是`f(X)`和`Y`的函数，那么变换视角，`L`就是关于`W`的函数。假设学习率为`η`，初始权重为`W0`，对于`D`中的任意`(X0,Y0)`，我们可以求``ΔW0=-η∇L(W0`(X0,Y0),f`)``。根据梯度的特性，我们可以知道对于更新后的权重`W1=W0+ΔW0`，有``L(W1`(X0,Y0),f`)<L(W0`(X0,Y0),f`)``。在适当的参数下，通过不断循环上述过程，就能得到最终的`W`。

那么``∇L(W`f`)``如何求呢？作为医学生，我们不必了解具体的数学原理来编程，只要构建好我们想要的网络结构`f`，[PyTorch](https://pytorch.org/)的[自动求导](https://www.codercto.com/a/81317.html#:~:text=PyTorch%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%EF%BC%88Autograd%EF%BC%89%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90%20%E6%88%91%E4%BB%AC%E7%9F%A5%E9%81%93%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%80%E6%A0%B8%E5%BF%83%E7%9A%84%E5%85%B6%E4%B8%AD%E4%B8%80%E4%B8%AA%E6%AD%A5%E9%AA%A4%EF%BC%8C%E5%B0%B1%E6%98%AF%E6%B1%82%E5%AF%BC%EF%BC%9A%E6%A0%B9%E6%8D%AE%E5%87%BD%E6%95%B0%EF%BC%88linear%20%2B%20activation%20function%EF%BC%89%E6%B1%82weights%E7%9B%B8%E5%AF%B9%E4%BA%8Eloss%E7%9A%84%E5%AF%BC%E6%95%B0%EF%BC%88%E8%BF%98%E6%98%AFloss%E7%9B%B8%E5%AF%B9%E4%BA%8Eweights%E7%9A%84%E5%AF%BC%E6%95%B0%EF%BC%9F,%EF%BC%89%E3%80%82%20%E7%84%B6%E5%90%8E%E6%A0%B9%E6%8D%AE%E5%BE%97%E5%87%BA%E7%9A%84%E5%AF%BC%E6%95%B0%EF%BC%8C%E7%9B%B8%E5%BA%94%E7%9A%84%E4%BF%AE%E6%94%B9weights%EF%BC%8C%E8%AE%A9loss%E6%9C%80%E5%B0%8F%E5%8C%96%E3%80%82%20%E5%90%84%E5%A4%A7%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6Tensorflow%EF%BC%8CKeras%EF%BC%8CPyTorch%E9%83%BD%E8%87%AA%E5%B8%A6%E6%9C%89%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E5%8A%9F%E8%83%BD%EF%BC%8C%E4%B8%8D%E9%9C%80%E8%A6%81%E6%88%91%E4%BB%AC%E6%89%8B%E5%8A%A8%E7%AE%97%E3%80%82%20%E5%9C%A8%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0PyTorch%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E7%9C%8B%E5%88%B0PyTorch%E7%9A%84%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E8%BF%87%E7%A8%8B%E6%97%B6%EF%BC%8C%E6%84%9F%E8%A7%89%E9%9D%9E%E5%B8%B8%E7%9A%84%E5%88%AB%E6%89%AD%E5%92%8C%E4%B8%8D%E7%9B%B4%E8%A7%82%E3%80%82%20%E6%88%91%E4%B8%8B%E9%9D%A2%E4%B8%BE%E4%B8%AA%E4%BE%8B%E5%AD%90%EF%BC%8C%E5%A4%A7%E5%AE%B6%E8%87%AA%E5%B7%B1%E6%84%9F%E5%8F%97%E4%B8%80%E4%B8%8B%E3%80%82)功能就可以自动帮我们计算出``∇L(W`f`)``啦。